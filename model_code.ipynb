{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrigank/misc/ml/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.neighbors.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/mrigank/misc/ml/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.linear_model.ridge module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/mrigank/misc/ml/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.weight_boosting module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/mrigank/misc/ml/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/mrigank/misc/ml/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "## models tried\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors.classification import KNeighborsClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "#### to be done\n",
    "\n",
    "# known\n",
    "from sklearn.linear_model.ridge import RidgeClassifier\n",
    "from sklearn.ensemble.weight_boosting import AdaBoostClassifier\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "\n",
    "# unknown\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#from sklearn.neighbors.classification import RadiusNeighborsClassifier\n",
    "#from sklearn.svm.classes import OneClassSVM\n",
    "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
    "#from sklearn.neighbors import NearestCentroid\n",
    "#from sklearn.svm import NuSVC\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "## mixture model ??\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions ##\n",
    "\n",
    "def getData(needScaled):\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    \n",
    "    \n",
    "    if (needScaled):\n",
    "        X_train = np.load('mmscaled_db_train.npy')\n",
    "        X_test = np.load('mmscaled_db_test.npy')\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        X_train = X_train[:,:-1]\n",
    "        X_test = X_test[:,:-1]\n",
    "        return X_train, X_test\n",
    "    else:\n",
    "        X_train = np.load('mmscaled_db_train.npy')\n",
    "        X_test = np.load('mmscaled_db_test.npy')\n",
    "        X_train = X_train[:,:-1]\n",
    "        X_test = X_test[:,:-1]\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "def printResults(clf_final, X_test, y_test):\n",
    "    y_pred = clf_final.predict(X_test)\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_loaded = np.load('db.npy')\n",
    "y = np_loaded[:, -1]\n",
    "X = np_loaded[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "X_train_unscaled,X_test_unscaled,y_train,y_test = [],[],[],[]\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train_unscaled, X_test_unscaled = X[train_index], X[test_index]\n",
    "    y_train, y_test= y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_scaled_loaded = np.load('scaled_db.npy')\n",
    "# y = np_scaled_loaded[:, -1]\n",
    "# X = np_scaled_loaded[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling ##\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_unscaled)\n",
    "X_train_scaled  = scaler.transform(X_train_unscaled)\n",
    "X_test_scaled = scaler.transform(X_test_unscaled)\n",
    "\n",
    "scaled_data_train = np.c_[X_train_scaled, y_train]\n",
    "np.save('mmscaled_db_train.npy', scaled_data_train)\n",
    "\n",
    "scaled_data_test = np.c_[X_test_scaled, y_test]\n",
    "np.save('mmscaled_db_test.npy', scaled_data_test)\n",
    "\n",
    "\n",
    "unscaled_data_train = np.c_[X_train_unscaled, y_train]\n",
    "np.save('unscaled_db_train.npy', unscaled_data_train)\n",
    "\n",
    "unscaled_data_test = np.c_[X_test_unscaled, y_test]\n",
    "np.save('unscaled_db_test.npy', unscaled_data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.load('unscaled_db_test.npy')\n",
    "# dic = {}\n",
    "# for i in data:\n",
    "#     yr = i[2]\n",
    "#     if yr in dic:\n",
    "#         dic[yr].append(i)\n",
    "#     else:\n",
    "#         dic[yr] = [i]\n",
    "# for yr in dic:\n",
    "#     s = str(int(yr))+\"testData\"\n",
    "#     np.save(s, np.array(dic[yr]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[3505    0]\n",
      " [   0 2468]]\n"
     ]
    }
   ],
   "source": [
    "# Gradient boosting classifier\n",
    "clf_final = GradientBoostingClassifier(n_estimators=100, learning_rate=0.9,random_state=0)\n",
    "clf_final.fit(X_train, y_train)\n",
    "y_pred = clf_final.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "'''\n",
    "0.7753222836095764\n",
    "[[2964  541]\n",
    " [ 801 1667]]\n",
    " \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree\n",
      "\n",
      "0.6360287962497907\n",
      "DecisionTreeClassifier(random_state=0)\n",
      "\n",
      "0.6360287962497907\n",
      "[[2440 1065]\n",
      " [1109 1359]]\n"
     ]
    }
   ],
   "source": [
    "## Decision Tree ##\n",
    "print(\"\")\n",
    "print(\"Decision Tree\")\n",
    "print(\"\")\n",
    "\n",
    "needScaled = False\n",
    "X_train, X_test = getData(needScaled)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "depth = [i for i in range(5, 16)]\n",
    "depth.append(None)\n",
    "min_samples_split = [i for i in range(2, 40, 3)]\n",
    "min_samples_leaf = [i for i in range(1,10)]\n",
    "param_dist = {\n",
    "    \"criterion\" : [\"gini\", \"entropy\"],\n",
    "    \"min_samples_split\" : min_samples_split,\n",
    "    \"min_samples_leaf\" : min_samples_leaf,\n",
    "    \"min_impurity_decrease\" : [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1],\n",
    "    \"max_depth\" : depth\n",
    "}\n",
    "grid = GridSearchCV(clf, param_grid=param_dist, n_jobs=-1, cv = 10)\n",
    "grid.fit(X_train, y_train)\n",
    "clf_final = grid.best_estimator_\n",
    "print(clf_final)\n",
    "f = open('DecisionTreePickle', 'wb') \n",
    "pickle.dump(clf_final, f)                      \n",
    "dbfile.close()\n",
    "\n",
    "print(\"\")\n",
    "printResults(clf_final, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest ##\n",
    "print(\"\")\n",
    "print(\"Random Forest\")\n",
    "print(\"\")\n",
    "needScaled = False\n",
    "X_train, X_test = getData(needScaled)\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "depth = [i for i in range(5, 16)]\n",
    "depth.append(None)\n",
    "n_estimators = [i for i in range(50, 200, 5)]\n",
    "param_dist = {\n",
    "    \"criterion\" : [\"gini\", \"entropy\"],\n",
    "    \"n_estimators\": n_estimators,\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"oob_score\": [True, False],\n",
    "    \"max_depth\" : depth\n",
    "}\n",
    "grid = GridSearchCV(clf, param_grid=param_dist, n_jobs=-1, cv = 10)\n",
    "grid.fit(X_train, y_train)\n",
    "clf_final = grid.best_estimator_\n",
    "print(clf_final)\n",
    "f = open('RandomForestPickle', 'wb') \n",
    "pickle.dump(clf_final, f)                      \n",
    "dbfile.close()\n",
    "\n",
    "print(\"\")\n",
    "printResults(clf_final, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression ##\n",
    "print(\"\")\n",
    "print(\"Logistic Regression\")\n",
    "print(\"\")\n",
    "needScaled = True\n",
    "X_train, X_test = getData(needScaled)\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "penalty = ['l1', 'l2', 'elasticnet']\n",
    "max_iter = [100, 500, 1000, 5000, 10000]\n",
    "C = [i/10 for i in range(1,20)]\n",
    "param_dist = {\n",
    "    \"penalty\" : penalty,\n",
    "    \"max_iter\" : max_iter,\n",
    "    \"C\":C,\n",
    "    \"fit_intercept\":[True, False],\n",
    "    \n",
    "    \n",
    "}\n",
    "grid = GridSearchCV(clf, param_grid=param_dist, n_jobs=-1, cv = 10)\n",
    "grid.fit(X_train, y_train)\n",
    "clf_final = grid.best_estimator_\n",
    "print(clf_final)\n",
    "f = open('LogisticRegressionPickle', 'wb') \n",
    "pickle.dump(clf_final, f)                      \n",
    "dbfile.close()\n",
    "\n",
    "print(\"\")\n",
    "printResults(clf_final, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVM ##\n",
    "print(\"\")\n",
    "print(\"SVM\")\n",
    "print(\"\")\n",
    "needScaled = True\n",
    "X_train, X_test = getData(needScaled)\n",
    "\n",
    "clf = SVC(random_state=0)\n",
    "max_iter = [100, 500, 1000, 5000, 10000]\n",
    "\n",
    "C = [i/10 for i in range(1,20)]\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']\n",
    "gammas = ['scale', 'auto', 0.05, 0.1, 0.15, 0.2]\n",
    "param_dist = {\n",
    "    \"kernel\" : kernels,\n",
    "    \"gamma\" : gammas,\n",
    "    \"C\" : C,\n",
    "    \"shrinking\" : [True, False],\n",
    "    \"max_iter\" : max_iter\n",
    "}\n",
    "grid = GridSearchCV(clf, param_grid=param_dist, n_jobs=-1, cv = 10)\n",
    "grid.fit(X_train, y_train)\n",
    "clf_final = grid.best_estimator_\n",
    "print(clf_final)\n",
    "f = open('SVMPickle', 'wb') \n",
    "pickle.dump(clf_final, f)                      \n",
    "dbfile.close()\n",
    "\n",
    "print(\"\")\n",
    "printResults(clf_final, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gaussian Naive Bayes\n",
      "\n",
      "0.6497572409174619\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## GNB ##\n",
    "print(\"\")\n",
    "print(\"Gaussian Naive Bayes\")\n",
    "print(\"\")\n",
    "needScaled = False\n",
    "\n",
    "X_train, X_test = getData(needScaled)\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "f = open('GNBPickleScaled', 'wb') \n",
    "pickle.dump(clf, f)                      \n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gaussian Naive Bayes\n",
      "\n",
      "0.6497572409174619\n"
     ]
    }
   ],
   "source": [
    "## GNB ##\n",
    "print(\"\")\n",
    "print(\"Gaussian Naive Bayes\")\n",
    "print(\"\")\n",
    "needScaled = True\n",
    "\n",
    "X_train, X_test = getData(needScaled)\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "f = open('GNBPickleUnscaled', 'wb') \n",
    "pickle.dump(clf, f)                      \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K Nearest Neighbors\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-eded17ed3224>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m }\n\u001b[0;32m     20\u001b[0m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mclf_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf_final\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    907\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    910\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    561\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## KNN ##\n",
    "print(\"\")\n",
    "print(\"K Nearest Neighbors\")\n",
    "print(\"\")\n",
    "needScaled = True\n",
    "\n",
    "X_train, X_test = getData(needScaled)\n",
    "clf = KNeighborsClassifier()\n",
    "n_neighbors = [2,4,6,8,10]\n",
    "algorithm = ['auto']\n",
    "weights = ['uniform', 'distance']\n",
    "leaf_size = [i for i in range(10, 51, 10)]\n",
    "\n",
    "param_dist = {\n",
    "    \"n_neighbors\" : n_neighbors,\n",
    "    \"algorithm\" : algorithm,\n",
    "    \"weights\" : weights,\n",
    "    \"leaf_size\" : leaf_size\n",
    "}\n",
    "grid = GridSearchCV(clf, param_grid=param_dist, n_jobs=-1, cv = 10)\n",
    "grid.fit(X_train, y_train)\n",
    "clf_final = grid.best_estimator_\n",
    "print(clf_final)\n",
    "f = open('KNNPickle', 'wb') \n",
    "pickle.dump(clf_final, f)                      \n",
    "f.close()\n",
    "\n",
    "print(\"\")\n",
    "printResults(clf_final, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perceptron\n",
      "\n",
      "Perceptron(max_iter=10000, penalty='l1')\n",
      "\n",
      "0.6224677716390423\n",
      "[[3485   20]\n",
      " [2235  233]]\n"
     ]
    }
   ],
   "source": [
    "## Perceptron ##\n",
    "print(\"\")\n",
    "print(\"Perceptron\")\n",
    "print(\"\")\n",
    "needScaled = False\n",
    "X_train, X_test = getData(needScaled)\n",
    "penalty = ['l2','l1','elasticnet']\n",
    "alpha = [0.0001*(10**i) for i in range(0,5)]\n",
    "max_iter = [10000]\n",
    "param_dist = {\n",
    "    \"penalty\" : penalty,\n",
    "    \"alpha\" : alpha,\n",
    "    \"max_iter\" : max_iter\n",
    "}\n",
    "\n",
    "clf = Perceptron()\n",
    "grid = GridSearchCV(clf, param_grid=param_dist, n_jobs=-1, cv = 10)\n",
    "grid.fit(X_train, y_train)\n",
    "clf_final = grid.best_estimator_\n",
    "print(clf_final)\n",
    "f = open('PerceptronPickle', 'wb') \n",
    "pickle.dump(clf_final, f)                      \n",
    "f.close()\n",
    "\n",
    "print(\"\")\n",
    "printResults(clf_final, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNNPickle\n",
      "KNeighborsClassifier(leaf_size=10, n_neighbors=10)\n",
      "0.6653273062112841\n",
      "[[2800  705]\n",
      " [1294 1174]]\n",
      "PerceptronPickle\n",
      "Perceptron(max_iter=10000, penalty='l1')\n",
      "0.6224677716390423\n",
      "[[3485   20]\n",
      " [2235  233]]\n",
      "GNBPickleScaled\n",
      "GaussianNB()\n",
      "0.6279926335174953\n",
      "[[2591  914]\n",
      " [1308 1160]]\n",
      "LogisticRegressionPickle\n",
      "LogisticRegression(C=0.2, fit_intercept=False, random_state=0)\n",
      "0.6854177130420225\n",
      "[[2896  609]\n",
      " [1270 1198]]\n",
      "RandomForestPickle\n",
      "RandomForestClassifier(bootstrap=False, max_depth=13, n_estimators=150,\n",
      "                       random_state=0)\n",
      "0.7264356269881131\n",
      "[[2996  509]\n",
      " [1125 1343]]\n",
      "DecisionTreePickle\n",
      "DecisionTreeClassifier(criterion='entropy', max_depth=10,\n",
      "                       min_impurity_decrease=0, min_samples_leaf=6,\n",
      "                       min_samples_split=38, random_state=0)\n",
      "0.7001506780512305\n",
      "[[2797  708]\n",
      " [1083 1385]]\n",
      "GNBPickleUnscaled\n",
      "GaussianNB()\n",
      "0.6243093922651933\n",
      "[[3125  380]\n",
      " [1864  604]]\n"
     ]
    }
   ],
   "source": [
    "l = {\"KNNPickle\":True, \"PerceptronPickle\":False, \"GNBPickleScaled\":True, \"LogisticRegressionPickle\":True, \"RandomForestPickle\":False, \"DecisionTreePickle\":False,\"GNBPickleUnscaled\":False}\n",
    "\n",
    "for i in l:\n",
    "    X_train, X_test = getData(l[i])\n",
    "    f = open(i, 'rb')\n",
    "    print(i)\n",
    "    model = pickle.load(f)\n",
    "    print(model)\n",
    "    printResults(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
